{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision trees\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Green' '1' 'Grape']\n",
      " ['Red' '2' 'Apple']\n",
      " ['Green' '1' 'Grape']\n",
      " ['Yellow' '2' 'Banana']\n",
      " ['Yellow' '2' 'Apple']\n",
      " ['Red' '1' 'Apple']]\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "\n",
    "training_data = [['Green', 'Red', 'Green', 'Yellow', 'Yellow', 'Red'], \n",
    "                [1, 2, 1, 2, 2, 1], \n",
    "                ['Grape', 'Apple', 'Grape', 'Banana', 'Apple', 'Apple']]\n",
    "\n",
    "training_data = np.array(training_data)\n",
    "training_data = np.transpose(training_data)\n",
    "print(training_data)\n",
    "\n",
    "features = []\n",
    "features.append('Color')\n",
    "features.append('Size')\n",
    "features.append('Fruit')\n",
    "\n",
    "def is_numeric(val):\n",
    "    return isinstance(val, int) or isinstance(val, float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the question class\n",
    "class Question(object):\n",
    "    \n",
    "    def __init__(self, feature, value):\n",
    "        self.feature = feature\n",
    "        self.value = value\n",
    "        \n",
    "    def match(self, example):\n",
    "        # Matching takes as input the value to be matched.\n",
    "        # Then checks the value against the value for the feature.\n",
    "        # If matched return true else false.\n",
    "        v = example[self.feature]\n",
    "        if not is_numeric(v):\n",
    "            try:\n",
    "                v = int(v)\n",
    "            except:\n",
    "                return self.value == v\n",
    "        return self.value == v\n",
    "    \n",
    "    def __repr__(self):\n",
    "        # This is just a helper method to print\n",
    "        # the question in a readable format.\n",
    "        condition = \"==\"\n",
    "        if is_numeric(self.value):\n",
    "            condition = \">=\"\n",
    "        return \"Is %s %s %s?\" % (\n",
    "            features[self.feature], condition, str(self.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Question(1, 2)\n",
    "a.match(training_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_counts(rows):\n",
    "    \"\"\"Counts the number of each type of example in a dataset.\"\"\"\n",
    "    counts = {}  # a dictionary of label -> count.\n",
    "    for row in rows:\n",
    "        # in our dataset format, the label is always the last column\n",
    "        label = row[-1]\n",
    "        if label not in counts:\n",
    "            counts[label] = 0\n",
    "        counts[label] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Apple': 3, 'Banana': 1, 'Grape': 2}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_vals(rows, col):\n",
    "    \"\"\"Find the unique values for a column in a dataset.\"\"\"\n",
    "    return set([row[col] for row in rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1', '2'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_vals(training_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the partitioning function\n",
    "\n",
    "'''\n",
    "\n",
    "Partitioning function partitions the rows into true rows and false rows\n",
    "according to the partition question.\n",
    "\n",
    "'''\n",
    "\n",
    "def partition(question, rows):\n",
    "    # Iterate over all the rows\n",
    "    true_rows = []\n",
    "    false_rows = []\n",
    "    for row in rows:\n",
    "        if question.match(row):\n",
    "            true_rows.append(row)\n",
    "        else:\n",
    "            false_rows.append(row)\n",
    "            \n",
    "    return true_rows, false_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True rows [array(['Red', '2', 'Apple'], dtype='<U6'), array(['Yellow', '2', 'Banana'], dtype='<U6'), array(['Yellow', '2', 'Apple'], dtype='<U6')]\n",
      "False rows [array(['Green', '1', 'Grape'], dtype='<U6'), array(['Green', '1', 'Grape'], dtype='<U6'), array(['Red', '1', 'Apple'], dtype='<U6')]\n"
     ]
    }
   ],
   "source": [
    "true, false = partition(a, training_data)\n",
    "print('True rows', true)\n",
    "print('False rows', false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(rows):\n",
    "    \"\"\"Calculate the Gini Impurity for a list of rows.\n",
    "\n",
    "    There are a few different ways to do this, I thought this one was\n",
    "    the most concise. See:\n",
    "    https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity\n",
    "    \"\"\"\n",
    "    # Get the counts for the different classes\n",
    "    counts = class_counts(rows)\n",
    "    impurity = 1\n",
    "    # Iterate over the classes\n",
    "    for lbl in counts:\n",
    "        # Get the fraction of items represented by the class\n",
    "        prob_of_lbl = counts[lbl] / float(len(rows))\n",
    "        # Calculate the probability square\n",
    "        impurity -= prob_of_lbl**2\n",
    "    return impurity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_mixing = [['Apple'], ['Apple']]\n",
    "gini(no_mixing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the information_gain for a particular question\n",
    "def info_gain(rows, question):\n",
    "    # Calculate the impurity for the initial rows\n",
    "    init_impurity = gini(rows)\n",
    "    # Partition the rows into true, false according to the question\n",
    "    true_rows, false_rows = partition(question, rows)\n",
    "    # Calculate the impuroity of the child nodes\n",
    "    true_impurity = gini(true_rows)\n",
    "    false_impurity = gini(false_rows)\n",
    "    # Calculate the weighted average\n",
    "    p = len(true_rows)/len(rows)\n",
    "    we_avg = p*true_impurity + (1-p)*false_impurity\n",
    "    information_gain = init_impurity - we_avg\n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.611111111111111"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demo:\n",
    "# Calculate the uncertainy of our training data.\n",
    "current_uncertainty = gini(training_data)\n",
    "current_uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1944444444444443"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How much information do we gain by partioning on 'Green'?\n",
    "q = Question(0, 'Red')\n",
    "info_gain(training_data, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the best split/question according to the information gain\n",
    "def find_best_split(rows):\n",
    "    # Iterate over the possible questions and features\n",
    "    best_question = None\n",
    "    best_ig = 0\n",
    "    for f in [0, 1]:\n",
    "        for v in unique_vals(rows, f):\n",
    "            # Generate the question\n",
    "            q = Question(f, v)\n",
    "            # Find the information gain for this question\n",
    "            ig = info_gain(rows, q)\n",
    "            if ig >= best_ig:\n",
    "                best_ig = ig\n",
    "                best_question = q\n",
    "    return best_question, best_ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Is Color == Green?, 0.36111111111111105)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best_split(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leaf:\n",
    "    \"\"\"A Leaf node classifies data.\n",
    "\n",
    "    This holds a dictionary of class (e.g., \"Apple\") -> number of times\n",
    "    it appears in the rows from the training data that reach this leaf.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rows):\n",
    "        self.predictions = class_counts(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision_Node:\n",
    "    \"\"\"A Decision Node asks a question.\n",
    "\n",
    "    This holds a reference to the question, and to the two child nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 question,\n",
    "                 true_branch,\n",
    "                 false_branch):\n",
    "        self.question = question\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we define the build tree function\n",
    "def build_tree(rows):\n",
    "    # Find the best question to ask at this node\n",
    "    question, info_gain = find_best_split(rows)\n",
    "    \n",
    "    if info_gain == 0:\n",
    "        return Leaf(rows)\n",
    "    \n",
    "    # Partition the rows according to the question\n",
    "    true_rows, false_rows = partition(question, rows)\n",
    "    \n",
    "    # Recursively build the tree for the true rows and the false rows\n",
    "    true_branch = build_tree(true_rows)\n",
    "    false_branch = build_tree(false_rows)\n",
    "    \n",
    "    return Decision_Node(question, true_branch, false_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(node, spacing=\"\"):\n",
    "    \"\"\"World's most elegant tree printing function.\"\"\"\n",
    "\n",
    "    # Base case: we've reached a leaf\n",
    "    if isinstance(node, Leaf):\n",
    "        print (spacing + \"Predict\", node.predictions)\n",
    "        return\n",
    "\n",
    "    # Print the question at this node\n",
    "    print (spacing + str(node.question))\n",
    "\n",
    "    # Call this function recursively on the true branch\n",
    "    print (spacing + '--> True:')\n",
    "    print_tree(node.true_branch, spacing + \"  \")\n",
    "\n",
    "    # Call this function recursively on the false branch\n",
    "    print (spacing + '--> False:')\n",
    "    print_tree(node.false_branch, spacing + \"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Color == Green?\n",
      "--> True:\n",
      "  Predict {'Grape': 2}\n",
      "--> False:\n",
      "  Is Color == Yellow?\n",
      "  --> True:\n",
      "    Predict {'Banana': 1, 'Apple': 1}\n",
      "  --> False:\n",
      "    Predict {'Apple': 2}\n"
     ]
    }
   ],
   "source": [
    "print_tree(build_tree(training_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will look at Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# Discriminative method for classification\n",
    "# Directly estimates the posterior probability\n",
    "\n",
    "EPS = 1e-12\n",
    "\n",
    "class LogisticRegression(object):\n",
    "    \n",
    "    def __init__(self, x, y, lr=0.001, ep=10):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.w = np.zeros((self.x.shape[1], 1))\n",
    "        self.num_classes = 3\n",
    "        self.learning_rate = lr\n",
    "        self.num_epochs = ep\n",
    "        \n",
    "    def loss_binary_grad(self, y_true, y_pred):\n",
    "        return -np.sum(np.dot(x.T, (y_true-y_pred)))\n",
    "    \n",
    "    def loss_cross_grad(self, y_true, y_pred):\n",
    "        return np.sum(y_pred-y_true)/(len(y_true))\n",
    "        \n",
    "    def sgd(self, y_true, y_pred):\n",
    "        # Use stochastic gradient descent to calculate the weights\n",
    "        loss_grad = self.loss_binary_grad(y_true, y_pred)\n",
    "        # Update the weights\n",
    "        self.w = self.w - self.learning_rate*(len(y_true)/len(self.y))*loss_grad\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        # For 2 class classification\n",
    "        return 1/(1+np.exp(-x)+EPS)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        # For numerical stability, subtract the max from x\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "    \n",
    "    def get_w_shapes(self):\n",
    "        return self.w.shape\n",
    "\n",
    "    def binary_crossentropy_loss(self, y_pred, y_true):\n",
    "        # Used for the 2 class problem\n",
    "        loss = np.sum(y_true*np.log(y_pred) + (1-y_true)*np.log(1-y_pred))\n",
    "        return loss\n",
    "    \n",
    "    def crossentropy_loss(self, y_pred, y_true):\n",
    "        return -np.sum(np.dot(y_true.T, np.log(y_pred)))/len(y_true)\n",
    "\n",
    "    def forward(self):\n",
    "        f_x = np.dot(self.x, self.w)\n",
    "        output = self.softmax(f_x)\n",
    "        return output\n",
    "    \n",
    "    def train(self):\n",
    "        for e in range(self.num_epochs):\n",
    "            y_pred = self.forward()\n",
    "            loss = self.crossentropy_loss(y_pred, y)\n",
    "            # Optimize\n",
    "            self.sgd(y, y_pred)\n",
    "            print(loss)\n",
    "        print('Training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['data']\n",
    "y = data['target']\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "y = onehotencoder.fit_transform(y.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.010635294096257"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the forward pass\n",
    "lr = LogisticRegression(x, y)\n",
    "y_pred = lr.forward()\n",
    "lr.crossentropy_loss(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "751.5952941144386\n",
      "2119.497158612277\n",
      "4027.956283982895\n",
      "5985.5369443652\n",
      "7955.205578495426\n",
      "9929.797654597536\n",
      "11906.94232539821\n",
      "13885.495295562467\n",
      "15864.829745834093\n",
      "17844.595854623614\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "lr.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
